import re
from dataclasses import dataclass
from typing import Any
from typing import TypeAlias

from clck.formulang.definitions.tokens import STANDARD_TOKENS, Literals, StandardTokens
from clck.formulang.definitions.tokens import VALID_CHARS
from clck.utils import clean_collection
from clck.utils import strip_whitespace


ResultName: TypeAlias = str


@dataclass()
class Token:
    type: StandardTokens
    value: Any
    brace_level: int

    def __repr__(self) -> str:
        if self.type.name == Literals.STRING_LITERAL.name:
            return f"<Token {self.type.name} \"{self.value}\">"
        else:
            return f"<Token {self.type.name} {self.value}>"

    def __str__(self) -> str:
        if self.type.name == Literals.STRING_LITERAL.name:
            return f"<Token {self.type.name} \"{self.value}\">"
        else:
            return f"<Token {self.type.name} {self.value}>"


class Tokenizer:
    """The class for `Tokenizer`.
    
    A `Tokenizer` allows interpretation of a given string formula before
    it can be passed on to a `Parser` object. It is an essential part of
    CLCK's lexing system and it usually handles the first phase of
    linguistic generation. An instance of this class also allows storage
    of relevant data about the analyzed string formula. `Tokenizer`s
    have an `analyze()` method that calls other tokenization methods of
    this class such as `get_string_len()`. 
    
    Below demonstrates simple code to retrieve tokenization data from a
    given string formula.::

        my_tokenizer = Tokenizer("abc")
        my_tokenizer.analyze()

    After a successful method call of `analyze()`, all relevant
    information are stored to the `result_data` dictionary attribute of
    the `Tokenizer` instance, accessible publicly especially for other
    lexing classes such as the `Parser` class. Results generated by
    `analyze()` can be accessed using the `get_result_data_by_name()`
    method.::

        my_tokenizer.get_result_data_by_name("string_length")
    """

    def __init__(self, formula: str):
        """Creates a new `Tokenizer` instance.

        Parameters
        ----------
        formula : str
            the string formula to feed this tokenizer with
        """

        self._formula: str = formula
        self._result_data: dict[ResultName, Any] = {}

    @property
    def formula(self) -> str:
        """The string formula stored to this tokenizer
        """

        return self._formula

    @property
    def result_data(self) -> dict[ResultName, Any]:
        """The dictionary containing the result data based on the most
        recent `analyze()` method call.

        All keys stored in this property are initially set to `None`
        after instantiation or after updating the formula string
        through `update_formula()`. Values are only generated and
        stored after calling `analyze()`.

        Each value in this dictionary property can be directly retrieved
        using `get_result_data_by_name()` or alternatively using the
        respective getter methods for each result data.
        """

        return self._result_data

    def analyze(self) -> None:
        """Analyzes this instance's formula string and stores each
        result to the property `result_data`.

        The following are the result data names and the values stored
        after calling this method.

        - `tokens` : the tuple of string retrieved after splitting the
            string formula into its component string tokens
        - `string_length` : the length of the formula string excluding
            all its whitespaces
        - `sequence_length` : the number of tokens recognized in the
            formula string
        """

        if self.are_formula_characters_valid():
            self._result_data["tokens"] = self._analyze_tokens()
            self._result_data["string_length"] = self._analyze_string_length()
            self._result_data["sequence_length"] = self._analyze_sequence_length()
        else:
            raise Exception("Invalid formula string detected")

    def are_formula_characters_valid(self) -> bool:
        """Checks for each character in the given string `formula`. This
        returns `False` if any of the checked characters is not present
        in `VALID_CHARS`, otherwise returns `True`.

        Returns
        -------
        bool
            `True` if all formula characters are valid, otherwise
            `False`
        """

        for char in strip_whitespace(self._formula):
            if char not in VALID_CHARS:
                raise Exception(f"Invalid character '{char}' found in formula string")
        return True

    def get_result_data_by_name(self, result_name: str) -> Any:
        """Returns the respective value for the requested key string,
        `result_name`.

        Parameters
        ----------
        result_name : str
            the key string of the requested result data

        Returns
        -------
        Any
            the value of the requested result data
        """

        try:
            return self._result_data[result_name]
        except KeyError:
            raise Exception(f"Non-existent result data name '{result_name}'. "
                + "It may not have been initialized yet, thus, consider "
                + "calling analyze() to initialize the result data "
                + "dictionary.")

    def get_tokens(self) -> tuple[Token, ...]:
        """Returns a tuple of `Tokens` retrieved from this tokenizer's
        formula string. This is a convenience method for directly
        retrieving this tokenizer's tokens as an alternative to calling
        `get_result_data_by_name()`.

        Returns
        -------
        tuple[str, ...]
            the tuple of Tokens
        """

        return self.get_result_data_by_name("tokens")
    
    def update_formula(self, new_formula: str) -> None:
        """Updates the formula string stored to this tokenizer and
        resets all result data by setting each dictionary entries to
        `None`.

        Parameters
        ----------
        new_formula : str
            the new formula string to be stored to this tokenizer
        """
        self._formula = new_formula
        self._result_data.clear()

    def _analyze_tokens(self) -> tuple[Token, ...]:
        """Returns a tuple of string values representing the tokens
        retrieved from this tokenizer's formula string.

        Returns
        -------
        tuple[str, ...]
            the tuple of string values representing the tokens
            retrieved
        """
        formula: str = strip_whitespace(self._formula)
        ret: list[Token] = []

        delimiter: str = self._get_delimiter_str()

        # Get the strings of tokens
        raw_tokens: tuple[str, ...] = clean_collection(re.split(rf"({delimiter}|\d+)", formula))

        brace_level: int = 0

        # Create Token objects from strings
        for token in raw_tokens:

            # Used to indicate closing brace levels
            if token in ("}", ")"):
                brace_level -= 1

            for token_definition in STANDARD_TOKENS:
                # Actually match the tokens to the definitions
                if re.match(token_definition.value, token):
                    ret.append(Token(token_definition, token, brace_level))
                    break

            # Used to indicate opening brace levels
            if token in ("{", "("):
                brace_level += 1

        return tuple(ret)

    def _analyze_string_length(self) -> int:
        """Returns the length of the formula string without all the
        whitespaces.

        Returns
        -------
        int
            the length of the formula string without all the whitespaces
        """

        return len(strip_whitespace(self._formula))

    def _analyze_sequence_length(self) -> int:
        """Returns the number of tokens recognized in this tokenizer's
        string formula.

        Returns
        -------
        int
            the number of tokens recognized in this tokenizer's string
            formula
        """
        return len(self.get_tokens())

    def _get_delimiter_str(self) -> str:
        """Returns the delimiter string of all the defined standard
        tokens in `clck.lexer.definitions.STANDARD_TOKENS`.

        Returns
        -------
        str
            the delimiter string based on the defined standard tokens
        """
        dls: list[str] = []
        for token_definition in STANDARD_TOKENS:
            dls.append(token_definition.value)
        dls.sort(key=len, reverse=True)
        d: str = "|".join(dls)

        return d