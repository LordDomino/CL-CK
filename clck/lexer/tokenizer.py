import re
from typing import Any, TypeAlias

from clck.lexer.definitions import SyntaxTokens, Wildcards
from clck.lexer.definitions import StandardTokens
from clck.lexer.definitions import VALID_CHARS
from clck.utils import clean_collection, strip_whitespace


ResultName: TypeAlias = str


class Tokenizer:
    """The class for `Tokenizer`.
    
    A `Tokenizer` allows interpretation of a given string formula before
    it can be passed on to a `Parser` object. It is an essential part of
    CLCK's lexing system and it usually handles the first phase of
    linguistic generation. An instance of this class also allows storage
    of relevant data about the analyzed string formula. `Tokenizer`s
    have an `analyze()` method that calls other tokenization methods of
    this class such as `get_string_len()`. 
    
    Below demonstrates simple code to retrieve tokenization data from a
    given string formula.::

        my_tokenizer = Tokenizer("abc")
        my_tokenizer.analyze()

    After a successful method call of `analyze()`, all relevant
    information are stored to the `result_data` dictionary attribute of
    the `Tokenizer` instance, accessible publicly especially for other
    lexing classes such as the `Parser` class. Results generated by
    `analyze()` can be accessed using the `get_result_data_by_name()`
    method.::

        my_tokenizer.get_result_data_by_name("string_length")
    """

    def __init__(self, formula: str):
        """Creates a new `Tokenizer` instance.

        Parameters
        ----------
        formula : str
            the string formula to feed this tokenizer with
        """

        self._formula: str = formula
        self._result_data: dict[ResultName, Any] = {}

    @property
    def formula(self) -> str:
        """The string formula stored to this tokenizer
        """

        return self._formula

    @property
    def result_data(self) -> dict[ResultName, Any]:
        """The dictionary containing the result data based on the most
        recent `analyze()` method call.

        All keys stored in this property are initially set to `None`
        after instantiation or after updating the formula string
        through `update_formula()`. Values are only generated and
        stored after calling `analyze()`.

        Each value in this dictionary property can be directly retrieved
        using `get_result_data_by_name()` or alternatively using the
        respective getter methods for each result data.
        """

        return self._result_data
    
    def _get_tokens_init(self) -> tuple[str, ...]:
        """Returns a tuple of string values representing the tokens
        retrieved from this tokenizer's formula string.

        Returns
        -------
        tuple[str, ...]
            the tuple of string values representing the tokens
            retrieved
        """
        formula: str = strip_whitespace(self._formula)
        delimiters: list[str] = []
        
        # Register syntax tokens as delimiters
        for token_str in StandardTokens.get_tokens_by_type(SyntaxTokens):
            delimiters.append(rf"\{token_str}")

        # Also register wildcards as delimiters,
        # but without the escape characters
        delimiters.extend(StandardTokens.get_tokens_by_type(Wildcards))

        delimiters.sort(key=len, reverse=True)
        delimiter: str = "|".join(delimiters)

        return clean_collection(tuple(re.split(rf"({delimiter}|\d+)", formula)))

    def analyze(self) -> None:
        """Analyzes this instance's formula string and stores each
        result to the property `result_data`.

        The following are the result data names and the values stored
        after calling this method.

        - `tokens` : the tuple of string retrieved after splitting the
            string formula into its component string tokens
        - `string_length` : the length of the formula string excluding
            all its whitespaces
        - `sequence_length` : the number of tokens recognized in the
            formula string
        """

        if self.are_formula_characters_valid():
            self._result_data["tokens"] = self._get_tokens_init()
            self._result_data["string_length"] = self.get_string_length()
            self._result_data["sequence_length"] = self.get_sequence_length()
        else:
            raise Exception("Invalid formula string detected")

    def are_formula_characters_valid(self) -> bool:
        """Checks for each character in the given string `formula`. This
        returns `False` if any of the checked characters is not present
        in `VALID_CHARS`, otherwise returns `True`.

        Returns
        -------
        bool
            `True` if all formula characters are valid, otherwise
            `False`
        """

        for char in strip_whitespace(self._formula):
            if char not in VALID_CHARS:
                raise Exception(f"Invalid character '{char}' found in formula string")
        return True
    
    def get_result_data_by_name(self, result_name: str) -> Any:
        """Returns the respective value for the requested key string,
        `result_name`.

        Parameters
        ----------
        result_name : str
            the key string of the requested result data

        Returns
        -------
        Any
            the value of the requested result data
        """

        try:
            return self._result_data[result_name]
        except KeyError:
            raise Exception(f"Non-existent result data name '{result_name}'")


    def get_string_length(self) -> int:
        """Returns the length of the formula string without all the
        whitespaces.

        Returns
        -------
        int
            the length of the formula string without all the whitespaces
        """

        return len(strip_whitespace(self._formula))

    def get_tokens(self) -> tuple[str, ...]:
        """Returns a tuple of string values representing the tokens
        retrieved from this tokenizer's formula string.

        Returns
        -------
        tuple[str, ...]
            the tuple of string values representing the tokens
            retrieved
        """
        try:
            return self._result_data["tokens"]
        except KeyError:
            raise Exception("Formula string is not yet analyzed!")

    def get_sequence_length(self) -> int:
        """Returns the number of tokens recognized in this tokenizer's
        string formula.

        Returns
        -------
        int
            the number of tokens recognized in this tokenizer's string
            formula
        """
        return len(self.get_tokens())
    
    def update_formula(self, new_formula: str) -> None:
        """Updates the formula string stored to this tokenizer and
        resets all result data by setting each dictionary entries to
        `None`.

        Parameters
        ----------
        new_formula : str
            the new formula string to be stored to this tokenizer
        """
        self._formula = new_formula

        for key in list(self._result_data.keys()):
            self._result_data[key] = None